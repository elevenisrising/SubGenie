#!/usr/bin/env python3
"""
ç®€åŒ–çš„åˆ†å¥é€»è¾‘ - æ›¿æ¢å¤æ‚çš„main.pyé€»è¾‘
======================================

æ ¸å¿ƒåŸåˆ™ï¼š
1. spaCyåŸºäºè¯­æ³•åˆ†å¥ï¼ˆæ›¿ä»£Whisperåˆ†å¥ï¼‰
2. å¼ºåˆ¶é•¿å¥æ£€æŸ¥ï¼ˆæ‰€æœ‰å¥å­éƒ½è¦æ£€æŸ¥ï¼‰
3. å¼ºåˆ¶æ—¶é—´æˆ³åˆ†é…ï¼ˆæ»‘åŠ¨çª—å£ç²¾ç¡®åŒ¹é…ï¼‰
4. Debugå®Œæ•´ï¼ˆåŒ¹é…ç‡ç»Ÿè®¡ï¼‰

æµç¨‹ï¼š
Whisperæ•°æ®æå– â†’ spaCyåˆ†å¥ â†’ é•¿å¥æ£€æŸ¥ â†’ æ—¶é—´æˆ³åˆ†é…
"""

import logging
import re
from typing import Dict, List, Tuple
import spacy

# åŠ è½½spaCyæ¨¡å‹
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    raise Exception("spaCy model not found. Install: python -m spacy download en_core_web_sm")


def structure_and_split_segments(transcription_result: Dict, max_chars: int, segmentation_strategy: str = "rule_based") -> List[Dict]:
    """
    å…¨æ–°ç®€åŒ–çš„åˆ†å¥æ¶æ„
    
    Args:
        transcription_result: Whisperè½¬å½•ç»“æœ
        max_chars: æœ€å¤§å­—ç¬¦é™åˆ¶  
        segmentation_strategy: åˆ†å¥ç­–ç•¥ï¼ˆå½“å‰éƒ½ç”¨spaCyï¼‰
        
    Returns:
        List[Dict]: å¤„ç†å¥½çš„å­—å¹•segments
    """
    logging.info("ğŸš€ Starting simplified segmentation logic...")
    
    # ç¬¬ä¸€æ­¥ï¼šæå–Whisperæ•°æ®
    whisper_data = extract_whisper_data(transcription_result)
    if not whisper_data['all_words']:
        logging.error("âŒ No word-level data from Whisper")
        return []
    
    # ç¬¬äºŒæ­¥ï¼šspaCyè¯­æ³•åˆ†å¥  
    sentences = spacy_sentence_segmentation(whisper_data['all_words'])
    if not sentences:
        logging.error("âŒ spaCy failed to generate sentences")
        return []
    
    logging.info(f"ğŸ“ spaCy generated {len(sentences)} initial sentences")
    
    # ç¬¬ä¸‰æ­¥ï¼šå¼ºåˆ¶é•¿å¥æ£€æŸ¥å’Œåˆ†å‰²
    checked_sentences = mandatory_long_sentence_check(sentences, max_chars, whisper_data)
    
    # ç¬¬å››æ­¥ï¼šå¼ºåˆ¶æ—¶é—´æˆ³åˆ†é…å’ŒéªŒè¯
    final_segments = mandatory_timestamp_assignment(checked_sentences, whisper_data)
    
    logging.info(f"âœ… Final result: {len(final_segments)} segments")
    return final_segments


def extract_whisper_data(transcription_result: Dict) -> Dict:
    """æå–Whisperçš„è¯çº§æ—¶é—´æˆ³å’ŒåŸå§‹segmentè¾¹ç•Œ"""
    whisper_segments = transcription_result.get("segments", [])
    if not whisper_segments:
        return {'all_words': [], 'segment_boundaries': []}
    
    all_words = []
    segment_boundaries = []
    
    for segment in whisper_segments:
        start_word_idx = len(all_words)
        segment_words = segment.get("words", [])
        
        if segment_words:  # åªå¤„ç†æœ‰è¯çš„segment
            all_words.extend(segment_words)
            end_word_idx = len(all_words) - 1
            
            segment_boundaries.append({
                'start_word_idx': start_word_idx,
                'end_word_idx': end_word_idx,
                'original_text': segment.get('text', '').strip(),
                'start_time': segment.get('start', 0.0),
                'end_time': segment.get('end', 0.0)
            })
    
    logging.info(f"ğŸ“Š Extracted {len(all_words)} words from {len(segment_boundaries)} Whisper segments")
    return {
        'all_words': all_words,
        'segment_boundaries': segment_boundaries
    }


def spacy_sentence_segmentation(all_words: List[Dict]) -> List[str]:
    """ä½¿ç”¨spaCyè¿›è¡Œè¯­æ³•åˆ†å¥ï¼ˆå¿½ç•¥Whisperåˆ†å¥ï¼‰"""
    
    # åˆå¹¶æ‰€æœ‰Whisper wordsæˆå®Œæ•´æ–‡æœ¬
    word_texts = []
    for word_info in all_words:
        word = word_info.get('word', '').strip()
        if word:
            word_texts.append(word)
    
    if not word_texts:
        logging.error("No valid words found")
        return []
    
    full_text = " ".join(word_texts)
    logging.info(f"ğŸ”¤ Combined text: {len(full_text)} chars")
    
    # spaCyè¯­æ³•åˆ†å¥
    try:
        doc = nlp(full_text)
        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
        logging.info(f"ğŸ“– spaCy detected {len(sentences)} sentences")
        return sentences
    except Exception as e:
        logging.error(f"âŒ spaCy processing failed: {e}")
        return []


def mandatory_long_sentence_check(sentences: List[str], max_chars: int, whisper_data: Dict) -> List[str]:
    """å¼ºåˆ¶é•¿å¥æ£€æŸ¥ - æ‰€æœ‰å¥å­éƒ½å¿…é¡»ç»è¿‡æ­¤æ­¥éª¤"""
    logging.info(f"\nğŸ” MANDATORY LONG SENTENCE CHECK (max_chars: {max_chars})")
    logging.info(f"   Checking {len(sentences)} sentences...")
    
    checked_sentences = []
    split_count = 0
    
    for i, sentence in enumerate(sentences):
        sentence = sentence.strip()
        if not sentence:
            continue
            
        logging.info(f"   ğŸ“ Sentence {i+1}: {len(sentence)} chars")
        logging.info(f"      Text: '{sentence[:60]}{'...' if len(sentence) > 60 else ''}'")
        
        if len(sentence) <= max_chars:
            logging.info(f"      âœ… Length OK")
            checked_sentences.append(sentence)
        else:
            logging.warning(f"      âš ï¸  TOO LONG! Splitting...")
            # åˆ†å‰²é•¿å¥
            split_parts = split_long_sentence(sentence, max_chars, whisper_data)
            checked_sentences.extend(split_parts)
            split_count += 1
            logging.info(f"      âœ‚ï¸  Split into {len(split_parts)} parts:")
            for j, part in enumerate(split_parts):
                logging.info(f"         {j+1}. ({len(part)} chars) '{part[:40]}{'...' if len(part) > 40 else ''}'")
    
    logging.info(f"ğŸ“Š Long sentence check complete:")
    logging.info(f"   ğŸ“¥ Input:  {len(sentences)} sentences")
    logging.info(f"   ğŸ“¤ Output: {len(checked_sentences)} sentences")
    logging.info(f"   âœ‚ï¸  Split:  {split_count} long sentences")
    
    return checked_sentences


def split_long_sentence(sentence: str, max_chars: int, whisper_data: Dict) -> List[str]:
    """åˆ†å‰²é•¿å¥ - å¤šç§ç­–ç•¥"""
    
    # ç­–ç•¥1: åœ¨Whisper segmentè¾¹ç•Œåˆ†å‰²ï¼ˆä¼˜å…ˆï¼‰
    whisper_splits = try_split_at_whisper_boundaries(sentence, max_chars, whisper_data)
    if len(whisper_splits) > 1 and all(len(part) <= max_chars for part in whisper_splits):
        logging.info(f"         ğŸ¯ Using Whisper boundaries")
        return whisper_splits
    
    # ç­–ç•¥2: è¯­æ³•ç‚¹åˆ†å‰²ï¼ˆè¿è¯ã€æ ‡ç‚¹ï¼‰
    grammar_splits = try_split_at_grammar_points(sentence, max_chars)
    if len(grammar_splits) > 1 and all(len(part) <= max_chars for part in grammar_splits):
        logging.info(f"         ğŸ“ Using grammar points")
        return grammar_splits
    
    # ç­–ç•¥3: å¼ºåˆ¶è¯è¾¹ç•Œåˆ†å‰²ï¼ˆæœ€åæ‰‹æ®µï¼‰
    word_splits = force_split_at_words(sentence, max_chars)
    logging.info(f"         âœ‚ï¸  Force split at words")
    return word_splits


def try_split_at_whisper_boundaries(sentence: str, max_chars: int, whisper_data: Dict) -> List[str]:
    """å°è¯•åœ¨WhisperåŸå§‹segmentè¾¹ç•Œåˆ†å‰²"""
    # ç®€åŒ–ç‰ˆæœ¬ï¼šå…ˆè¿”å›åŸå¥ï¼Œåç»­å¯ä»¥å®ç°Whisperè¾¹ç•Œé€»è¾‘
    # TODO: å®ç°Whisperè¾¹ç•Œæ£€æµ‹å’Œåˆ†å‰²
    return [sentence]


def try_split_at_grammar_points(sentence: str, max_chars: int) -> List[str]:
    """åœ¨è¯­æ³•ç‚¹åˆ†å‰²ï¼šè¿è¯ã€é€—å·ã€åˆ†å·ç­‰"""
    
    # å®šä¹‰åˆ†å‰²ç‚¹ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    split_patterns = [
        r'\b(and|but|or|so|because|since|while|although|however|therefore|meanwhile)\b',
        r'[,;:]',
        r'\.',
    ]
    
    for pattern in split_patterns:
        matches = list(re.finditer(pattern, sentence, re.IGNORECASE))
        
        if matches:
            splits = []
            last_pos = 0
            
            for match in matches:
                split_pos = match.end()  # åœ¨åŒ¹é…ååˆ†å‰²
                part = sentence[last_pos:split_pos].strip()
                
                if len(part) >= 20:  # é¿å…å¤ªçŸ­çš„ç‰‡æ®µ
                    if len(part) <= max_chars:
                        splits.append(part)
                        last_pos = split_pos
                    else:
                        break  # è¿™ä¸ªç­–ç•¥ä¸é€‚ç”¨
            
            # æ·»åŠ å‰©ä½™éƒ¨åˆ†
            if last_pos < len(sentence):
                remaining = sentence[last_pos:].strip()
                if remaining:
                    splits.append(remaining)
            
            # æ£€æŸ¥æ‰€æœ‰éƒ¨åˆ†éƒ½ç¬¦åˆé•¿åº¦è¦æ±‚
            if len(splits) > 1 and all(len(part) <= max_chars for part in splits):
                return splits
    
    return [sentence]


def force_split_at_words(sentence: str, max_chars: int) -> List[str]:
    """å¼ºåˆ¶åœ¨è¯è¾¹ç•Œåˆ†å‰²ï¼ˆä¿è¯ç»“æœï¼‰"""
    words = sentence.split()
    if not words:
        return [sentence]
    
    splits = []
    current_segment = []
    current_length = 0
    
    for word in words:
        word_length = len(word)
        space_length = 1 if current_segment else 0
        total_length = current_length + space_length + word_length
        
        if total_length > max_chars and current_segment:
            # å¼€å§‹æ–°segment
            splits.append(' '.join(current_segment))
            current_segment = [word]
            current_length = word_length
        else:
            current_segment.append(word)
            current_length = total_length
    
    # æ·»åŠ æœ€åä¸€ä¸ªsegment
    if current_segment:
        splits.append(' '.join(current_segment))
    
    return splits


def mandatory_timestamp_assignment(sentences: List[str], whisper_data: Dict) -> List[Dict]:
    """å¼ºåˆ¶æ—¶é—´æˆ³åˆ†é… - æ»‘åŠ¨çª—å£ç²¾ç¡®åŒ¹é… + DebugéªŒè¯"""
    logging.info(f"\nâ±ï¸  MANDATORY TIMESTAMP ASSIGNMENT")
    logging.info(f"   Assigning timestamps to {len(sentences)} sentences...")
    
    all_words = whisper_data['all_words']
    if not all_words:
        logging.error("âŒ No word data for timestamp assignment")
        return []
    
    # åˆ›å»ºæ¸…ç†åçš„è¯åºåˆ—ç”¨äºåŒ¹é…
    clean_word_sequence = []
    for i, word_info in enumerate(all_words):
        original_word = word_info.get('word', '').strip()
        if original_word:
            # æ¸…ç†æ ‡ç‚¹ä½†ä¿ç•™ç¼©å†™ï¼ˆI'm, don'tç­‰ï¼‰
            clean_word = re.sub(r'[^\w\s\'-]', '', original_word).lower().strip()
            if clean_word:
                clean_word_sequence.append({
                    'clean_word': clean_word,
                    'original_word': original_word,
                    'start': word_info.get('start', 0.0),
                    'end': word_info.get('end', 0.0)
                })
        
        # æ˜¾ç¤ºå¤„ç†è¿›åº¦
        if (i + 1) % 100 == 0 or i == len(all_words) - 1:
            logging.info(f"   ğŸ”„ Processing words: {i + 1}/{len(all_words)}")
    
    logging.info(f"   ğŸ“Š {len(clean_word_sequence)} words available for matching")
    
    final_segments = []
    used_word_indices = set()
    perfect_matches = 0
    good_matches = 0
    poor_matches = 0
    
    # é€å¥åŒ¹é…
    for i, sentence in enumerate(sentences):
        # logging.info(f"\n   ğŸ¯ Matching sentence {i+1}/{len(sentences)}")
        # logging.info(f"      Text: '{sentence[:50]}{'...' if len(sentence) > 50 else ''}'")
        
        # ç®€åŒ–è¿›åº¦æ˜¾ç¤º
        if (i + 1) % 10 == 0 or i == len(sentences) - 1:
            logging.info(f"   ğŸ¯ Matching sentences: {i + 1}/{len(sentences)}")
        
        # æ»‘åŠ¨çª—å£åŒ¹é…
        start_idx, end_idx, confidence = sliding_window_match(sentence, clean_word_sequence, used_word_indices)
        
        if start_idx != -1 and end_idx != -1:
            # æ ‡è®°å·²ä½¿ç”¨çš„è¯
            for idx in range(start_idx, end_idx + 1):
                used_word_indices.add(idx)
            
            # åˆ›å»ºsegment
            segment = {
                'text': sentence,
                'start': clean_word_sequence[start_idx]['start'],
                'end': clean_word_sequence[end_idx]['end'],
                'word_count': end_idx - start_idx + 1,
                'timing_confidence': confidence
            }
            final_segments.append(segment)
            
            # ç»Ÿè®¡åŒ¹é…è´¨é‡
            if confidence == 1.0:
                perfect_matches += 1
                # emoji = "âœ…"
            elif confidence >= 0.8:
                good_matches += 1
                # emoji = "ğŸŸ¡"
            else:
                poor_matches += 1
                # emoji = "ğŸŸ "
            
            # æ³¨é‡Šæ‰è¯¦ç»†åŒ¹é…ä¿¡æ¯
            # logging.info(f"      {emoji} Match: words [{start_idx}-{end_idx}], confidence: {confidence:.2f}")
            # logging.info(f"         Time: {segment['start']:.2f}s - {segment['end']:.2f}s")
        else:
            # åŒ¹é…å¤±è´¥ï¼Œåˆ›å»ºfallback segmentå¹¶è¾“å‡ºè¯¦ç»†debugä¿¡æ¯
            logging.error(f"\nâŒ MATCH FAILED for sentence {i+1}/{len(sentences)}:")
            logging.error(f"   åŸå¥: '{sentence}'")
            
            # æå–å¥å­çš„æ¸…ç†è¯ç”¨äºdebug
            sentence_clean_words = []
            for word in sentence.split():
                clean_word = re.sub(r'[^\w\s\'-]', '', word).lower().strip()
                if clean_word:
                    sentence_clean_words.append(clean_word)
            
            logging.error(f"   å¥å­æ¸…ç†åçš„è¯: {sentence_clean_words}")
            
            # æ˜¾ç¤ºé™„è¿‘å¯ç”¨çš„è¯åºåˆ—ç”¨äºåˆ†æ
            available_start = max(0, len([idx for idx in range(len(clean_word_sequence)) if idx not in used_word_indices]) - 10)
            available_words = [clean_word_sequence[i]['clean_word'] for i in range(len(clean_word_sequence)) if i not in used_word_indices]
            if available_words:
                logging.error(f"   å¯ç”¨è¯åºåˆ—(å‰20ä¸ª): {available_words[:20]}")
            else:
                logging.error(f"   âš ï¸  æ‰€æœ‰è¯éƒ½å·²è¢«ä½¿ç”¨!")
            
            fallback_segment = {
                'text': sentence,
                'start': 0.0,
                'end': 0.0,
                'word_count': len(sentence.split()),
                'timing_confidence': 0.0
            }
            final_segments.append(fallback_segment)
            poor_matches += 1
    
    # ç»Ÿè®¡å’ŒæŠ¥å‘Š
    total_sentences = len(sentences)
    success_rate = (perfect_matches + good_matches) / total_sentences if total_sentences > 0 else 0
    
    logging.info(f"\nğŸ“Š TIMESTAMP MATCHING RESULTS:")
    logging.info(f"   ğŸ¯ Perfect matches (1.0):   {perfect_matches}/{total_sentences}")
    logging.info(f"   ğŸŸ¡ Good matches (â‰¥0.8):     {good_matches}/{total_sentences}")
    logging.info(f"   ğŸŸ  Poor matches (<0.8):     {poor_matches}/{total_sentences}")
    logging.info(f"   âŒ Failed matches (0.0):    {total_sentences - perfect_matches - good_matches - poor_matches}/{total_sentences}")
    logging.info(f"   ğŸ“ˆ Overall success rate:    {success_rate:.1%}")
    
    if success_rate == 1.0:
        logging.info(f"   ğŸ‰ PERFECT! All sentences matched with high confidence!")
    elif success_rate >= 0.9:
        logging.info(f"   âœ… Excellent matching quality")
    elif success_rate >= 0.8:
        logging.info(f"   ğŸŸ¡ Good matching quality")
    else:
        logging.warning(f"   âš ï¸  Poor matching quality - may need debugging")
    
    return final_segments


def sliding_window_match(sentence: str, word_sequence: List[Dict], used_indices: set) -> Tuple[int, int, float]:
    """æ»‘åŠ¨çª—å£åŒ¹é…ç®—æ³• - ç²¾ç¡®åŒ¹é…å¥å­åˆ°è¯åºåˆ—"""
    
    # æå–å¥å­ä¸­çš„æ¸…ç†è¯
    sentence_words = []
    for word in sentence.split():
        clean_word = re.sub(r'[^\w\s\'-]', '', word).lower().strip()
        if clean_word:
            sentence_words.append(clean_word)
    
    if not sentence_words:
        return -1, -1, 0.0
    
    # logging.info(f"         ğŸ” Searching for {len(sentence_words)} words: {sentence_words[:5]}{'...' if len(sentence_words) > 5 else ''}")
    
    best_start = -1
    best_end = -1
    best_confidence = 0.0
    
    # æ»‘åŠ¨çª—å£æœç´¢
    search_end = len(word_sequence) - len(sentence_words) + 1
    for start_idx in range(search_end):
        end_idx = start_idx + len(sentence_words) - 1
        
        # è·³è¿‡å·²ä½¿ç”¨çš„åŒºåŸŸ
        if any(idx in used_indices for idx in range(start_idx, end_idx + 1)):
            continue
        
        # è®¡ç®—åŒ¹é…åº¦
        matches = 0
        mismatches = []
        
        for i, sentence_word in enumerate(sentence_words):
            word_idx = start_idx + i
            if word_idx < len(word_sequence):
                if word_sequence[word_idx]['clean_word'] == sentence_word:
                    matches += 1
                else:
                    mismatches.append(f"{sentence_word}â‰ {word_sequence[word_idx]['clean_word']}")
        
        confidence = matches / len(sentence_words)
        
        # å®Œç¾åŒ¹é…ï¼Œç«‹å³è¿”å›
        if confidence == 1.0:
            # logging.info(f"         ğŸ¯ Perfect match found at words [{start_idx}-{end_idx}]")
            return start_idx, end_idx, confidence
        
        # è®°å½•æœ€ä½³éƒ¨åˆ†åŒ¹é…
        if confidence > best_confidence:
            best_confidence = confidence
            best_start = start_idx
            best_end = end_idx
    
    # åªæ¥å—é«˜ç½®ä¿¡åº¦çš„åŒ¹é…
    if best_confidence >= 0.8:
        # logging.info(f"         ğŸŸ¡ Good partial match: {best_confidence:.2f} at words [{best_start}-{best_end}]")
        return best_start, best_end, best_confidence
    elif best_confidence > 0.0:
        # logging.warning(f"         ğŸŸ  Poor match: {best_confidence:.2f} (rejected)")
        return -1, -1, best_confidence
    else:
        # logging.error(f"         âŒ No match found")
        return -1, -1, 0.0


if __name__ == "__main__":
    print("ç®€åŒ–åˆ†å¥é€»è¾‘å·²å‡†å¤‡å¥½ï¼")
    print("ç‰¹ç‚¹ï¼š")
    print("âœ… spaCyè¯­æ³•åˆ†å¥ï¼ˆé«˜è´¨é‡ï¼‰")
    print("âœ… å¼ºåˆ¶é•¿å¥æ£€æŸ¥ï¼ˆæ‰€æœ‰å¥å­ï¼‰")
    print("âœ… å¼ºåˆ¶æ—¶é—´æˆ³åˆ†é…ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰")
    print("âœ… å®Œæ•´Debugä¿¡æ¯ï¼ˆåŒ¹é…ç‡ç»Ÿè®¡ï¼‰")
    print("âœ… æ¶æ„ç®€æ´ï¼ˆæ— è¡¥ä¸ä»£ç ï¼‰")